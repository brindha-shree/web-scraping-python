# web-scraping-python
A web scraping project using Python and  BeautifulSoup to extract data from wikipedia
Hereâ€™s a **README.md** file for your **Wikipedia Web Scraping Project** that you can upload to GitHub.  

---

# **ğŸ“Œ Wikipedia Web Scraping | Largest US Companies by Revenue**  

## **ğŸ“ Project Overview**  
This project scrapes data from **Wikipedia's "List of Largest Companies in the United States by Revenue"** page using **Python, BeautifulSoup, and Pandas**. The extracted data is cleaned and stored in a structured format for analysis.  

---

## **ğŸš€ Technologies Used**  
- **Python** ğŸ  
- **BeautifulSoup** (for web scraping)  
- **Requests** (for HTTP requests)  
- **Pandas** (for data handling & exporting)  

---

## **ğŸ“‚ Project Structure**  
```
wikipedia-web-scraping/
â”‚â”€â”€ data/                   # Folder for storing scraped data
â”‚   â”œâ”€â”€ Companies.csv       # Output CSV file
â”‚â”€â”€ scripts/                # Python script for scraping
â”‚   â”œâ”€â”€ scraper.py          # Main web scraping script
â”‚â”€â”€ README.md               # Project documentation
â”‚â”€â”€ requirements.txt        # Python dependencies
```

---

## **âš™ï¸ Setup & Installation**  
### **1ï¸âƒ£ Clone the Repository**  
```bash
git clone https://github.com/your-username/wikipedia-web-scraping.git
cd wikipedia-web-scraping
```

### **2ï¸âƒ£ Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Run the Scraper**  
```bash
python scripts/scraper.py
```

---

## **ğŸ“Š Sample Output (First 5 Rows)**  
| Rank | Name  | Industry | Revenue (USD billions) | Employees | Headquarters |  
|------|------|----------|------------------|------------|--------------|  
| 1    | Walmart | Retail | $611.3 | 2,300,000 | Bentonville, Arkansas |  
| 2    | Amazon  | E-commerce | $502.2 | 1,540,000 | Seattle, Washington |  
| 3    | ExxonMobil | Oil & Gas | $413.7 | 62,000 | Irving, Texas |  

ğŸ“Œ The full dataset is saved as **Companies.csv** in the `data/` folder.  

---

## **ğŸ“Œ Key Features**  
âœ” Scrapes **real-time data** from Wikipedia.  
âœ” Extracts structured tabular data using **BeautifulSoup**.  
âœ” Cleans and **saves data in CSV format** using Pandas.  
âœ” Can be extended to **scrape other Wikipedia tables**.  



